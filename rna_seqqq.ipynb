{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gzip\n",
    "import urllib.request\n",
    "from io import StringIO\n",
    "\n",
    "# ML and stats libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "\n",
    "# Bioinformatics libraries\n",
    "try:\n",
    "    import pysam\n",
    "    import HTSeq\n",
    "except ImportError:\n",
    "    print(\"Warning: Some bioinformatics libraries not available. Install with: pip install pysam HTSeq\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FungalRNAseqMLPipeline:\n",
    "    \"\"\"Complete pipeline for fungal RNA-seq ML analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, project_dir=\"yeast_rnaseq_ml_project\", threads=8):\n",
    "        self.project_dir = Path(project_dir)\n",
    "        self.threads = threads\n",
    "        \n",
    "        # Create directory structure\n",
    "        self.dirs = {\n",
    "            'data': self.project_dir / 'data',\n",
    "            'fastq': self.project_dir / 'data' / 'fastq',\n",
    "            'processed': self.project_dir / 'data' / 'processed',\n",
    "            'trimmed': self.project_dir / 'data' / 'trimmed',\n",
    "            'aligned': self.project_dir / 'data' / 'aligned',\n",
    "            'counts': self.project_dir / 'data' / 'counts',\n",
    "            'references': self.project_dir / 'references',\n",
    "            'results': self.project_dir / 'results',\n",
    "            'ml_results': self.project_dir / 'results' / 'ml_analysis',\n",
    "            'plots': self.project_dir / 'results' / 'plots',\n",
    "            'logs': self.project_dir / 'logs'\n",
    "        }\n",
    "        \n",
    "        for dir_path in self.dirs.values():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.dirs['logs'] / 'pipeline.log'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Sample metadata\n",
    "        self.sample_info = {\n",
    "            'SRR9929264': {'condition': 'Heat', 'replicate': 1, 'paired': True},\n",
    "            'SRR9929266': {'condition': 'Heat', 'replicate': 2, 'paired': True},\n",
    "            'SRR14139686': {'condition': 'Osmotic', 'replicate': 1, 'paired': True},\n",
    "            'SRR14139687': {'condition': 'Osmotic', 'replicate': 2, 'paired': True},\n",
    "            'SRR14808564': {'condition': 'Starvation', 'replicate': 1, 'paired': True},\n",
    "            'SRR14808566': {'condition': 'Starvation', 'replicate': 2, 'paired': True},\n",
    "            'SRR636633': {'condition': 'Oxidative', 'replicate': 1, 'paired': True},\n",
    "            'SRR636636': {'condition': 'Oxidative', 'replicate': 2, 'paired': True}\n",
    "        }\n",
    "        \n",
    "        # Reference URLs\n",
    "        self.reference_urls = {\n",
    "            'genome_fasta': 'ftp://ftp.ensembl.org/pub/release-107/fasta/saccharomyces_cerevisiae/dna/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz',\n",
    "            'cdna_fasta': 'ftp://ftp.ensembl.org/pub/release-107/fasta/saccharomyces_cerevisiae/cdna/Saccharomyces_cerevisiae.R64-1-1.cdna.all.fa.gz',\n",
    "            'gtf': 'ftp://ftp.ensembl.org/pub/release-107/gtf/saccharomyces_cerevisiae/Saccharomyces_cerevisiae.R64-1-1.107.gtf.gz'\n",
    "        }\n",
    "        \n",
    "        self.logger.info(f\"Pipeline initialized. Project directory: {self.project_dir}\")\n",
    "    \n",
    "    def check_dependencies(self):\n",
    "        \"\"\"Check if required tools are installed\"\"\"\n",
    "        required_tools = ['fastq-dump', 'salmon', 'fastqc', 'cutadapt', 'multiqc']\n",
    "        missing_tools = []\n",
    "        \n",
    "        for tool in required_tools:\n",
    "            try:\n",
    "                subprocess.run([tool, '--version'], capture_output=True, check=True)\n",
    "                self.logger.info(f\"✓ {tool} is available\")\n",
    "            except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "                missing_tools.append(tool)\n",
    "                self.logger.warning(f\"✗ {tool} is not available\")\n",
    "        \n",
    "        if missing_tools:\n",
    "            self.logger.error(f\"Missing tools: {missing_tools}\")\n",
    "            self.logger.error(\"Please install using conda:\")\n",
    "            self.logger.error(\"conda install -c bioconda sra-tools salmon fastqc cutadapt multiqc\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def download_references(self):\n",
    "        \"\"\"Download reference genome and annotation files\"\"\"\n",
    "        self.logger.info(\"Downloading reference files...\")\n",
    "        \n",
    "        for name, url in self.reference_urls.items():\n",
    "            local_path = self.dirs['references'] / Path(url).name\n",
    "            decompressed_path = local_path.with_suffix('')\n",
    "            \n",
    "            if decompressed_path.exists():\n",
    "                self.logger.info(f\"Reference file {name} already exists\")\n",
    "                continue\n",
    "            \n",
    "            self.logger.info(f\"Downloading {name} from {url}\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, local_path)\n",
    "                \n",
    "                # Decompress\n",
    "                with gzip.open(local_path, 'rb') as f_in:\n",
    "                    with open(decompressed_path, 'wb') as f_out:\n",
    "                        f_out.write(f_in.read())\n",
    "                \n",
    "                local_path.unlink()  # Remove compressed file\n",
    "                self.logger.info(f\"Downloaded and decompressed {name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to download {name}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def download_sra_data(self):\n",
    "        \"\"\"Download SRA data using fastq-dump\"\"\"\n",
    "        self.logger.info(\"Downloading SRA data...\")\n",
    "        \n",
    "        for srr, info in self.sample_info.items():\n",
    "            fastq_files = [\n",
    "                self.dirs['fastq'] / f\"{srr}_1.fastq\",\n",
    "                self.dirs['fastq'] / f\"{srr}_2.fastq\" if info['paired'] else None\n",
    "            ]\n",
    "            \n",
    "            if all(f.exists() for f in fastq_files if f is not None):\n",
    "                self.logger.info(f\"FASTQ files for {srr} already exist\")\n",
    "                continue\n",
    "            \n",
    "            self.logger.info(f\"Downloading {srr}...\")\n",
    "            cmd = [\n",
    "                'fastq-dump',\n",
    "                '--outdir', str(self.dirs['fastq']),\n",
    "                '--split-files' if info['paired'] else '--split-spot',\n",
    "                '--skip-technical',\n",
    "                '--readids',\n",
    "                '--read-filter', 'pass',\n",
    "                '--dumpbase',\n",
    "                '--clip',\n",
    "                srr\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "                self.logger.info(f\"Successfully downloaded {srr}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                self.logger.error(f\"Failed to download {srr}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def quality_control(self):\n",
    "        \"\"\"Run FastQC on all FASTQ files\"\"\"\n",
    "        self.logger.info(\"Running quality control...\")\n",
    "        \n",
    "        qc_dir = self.dirs['results'] / 'qc'\n",
    "        qc_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        fastq_files = list(self.dirs['fastq'].glob('*.fastq'))\n",
    "        \n",
    "        if not fastq_files:\n",
    "            self.logger.error(\"No FASTQ files found for QC\")\n",
    "            return False\n",
    "        \n",
    "        cmd = [\n",
    "            'fastqc',\n",
    "            '-t', str(self.threads),\n",
    "            '-o', str(qc_dir)\n",
    "        ] + [str(f) for f in fastq_files]\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(cmd, check=True)\n",
    "            self.logger.info(\"FastQC completed successfully\")\n",
    "            \n",
    "            # Run MultiQC to aggregate results\n",
    "            subprocess.run(['multiqc', str(qc_dir), '-o', str(qc_dir)], check=True)\n",
    "            self.logger.info(\"MultiQC completed successfully\")\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"Quality control failed: {e}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def trim_adapters(self):\n",
    "        \"\"\"Trim adapters and low-quality sequences\"\"\"\n",
    "        self.logger.info(\"Trimming adapters...\")\n",
    "        \n",
    "        for srr, info in self.sample_info.items():\n",
    "            if info['paired']:\n",
    "                input_files = [\n",
    "                    self.dirs['fastq'] / f\"{srr}_1.fastq\",\n",
    "                    self.dirs['fastq'] / f\"{srr}_2.fastq\"\n",
    "                ]\n",
    "                output_files = [\n",
    "                    self.dirs['trimmed'] / f\"{srr}_1.trimmed.fastq\",\n",
    "                    self.dirs['trimmed'] / f\"{srr}_2.trimmed.fastq\"\n",
    "                ]\n",
    "            else:\n",
    "                input_files = [self.dirs['fastq'] / f\"{srr}.fastq\"]\n",
    "                output_files = [self.dirs['trimmed'] / f\"{srr}.trimmed.fastq\"]\n",
    "            \n",
    "            if all(f.exists() for f in output_files):\n",
    "                self.logger.info(f\"Trimmed files for {srr} already exist\")\n",
    "                continue\n",
    "            \n",
    "            self.logger.info(f\"Trimming {srr}...\")\n",
    "            \n",
    "            if info['paired']:\n",
    "                cmd = [\n",
    "                    'cutadapt',\n",
    "                    '-j', str(self.threads),\n",
    "                    '-a', 'AGATCGGAAGAGC',  # TruSeq adapter\n",
    "                    '-A', 'AGATCGGAAGAGC',\n",
    "                    '-q', '20',\n",
    "                    '-m', '20',\n",
    "                    '--max-n', '0.1',\n",
    "                    '-o', str(output_files[0]),\n",
    "                    '-p', str(output_files[1]),\n",
    "                    str(input_files[0]),\n",
    "                    str(input_files[1])\n",
    "                ]\n",
    "            else:\n",
    "                cmd = [\n",
    "                    'cutadapt',\n",
    "                    '-j', str(self.threads),\n",
    "                    '-a', 'AGATCGGAAGAGC',\n",
    "                    '-q', '20',\n",
    "                    '-m', '20',\n",
    "                    '--max-n', '0.1',\n",
    "                    '-o', str(output_files[0]),\n",
    "                    str(input_files[0])\n",
    "                ]\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "                self.logger.info(f\"Successfully trimmed {srr}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                self.logger.error(f\"Failed to trim {srr}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def build_salmon_index(self):\n",
    "        \"\"\"Build Salmon index for quantification\"\"\"\n",
    "        self.logger.info(\"Building Salmon index...\")\n",
    "        \n",
    "        index_dir = self.dirs['references'] / 'salmon_index'\n",
    "        cdna_fasta = self.dirs['references'] / 'Saccharomyces_cerevisiae.R64-1-1.cdna.all.fa'\n",
    "        \n",
    "        if index_dir.exists():\n",
    "            self.logger.info(\"Salmon index already exists\")\n",
    "            return True\n",
    "        \n",
    "        cmd = [\n",
    "            'salmon', 'index',\n",
    "            '-t', str(cdna_fasta),\n",
    "            '-i', str(index_dir),\n",
    "            '--type', 'quasi',\n",
    "            '-k', '31'\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "            self.logger.info(\"Salmon index built successfully\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"Failed to build Salmon index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def quantify_expression(self):\n",
    "        \"\"\"Quantify gene expression with Salmon\"\"\"\n",
    "        self.logger.info(\"Quantifying gene expression...\")\n",
    "        \n",
    "        index_dir = self.dirs['references'] / 'salmon_index'\n",
    "        \n",
    "        for srr, info in self.sample_info.items():\n",
    "            sample_name = f\"{info['condition']}_Rep{info['replicate']}\"\n",
    "            output_dir = self.dirs['counts'] / sample_name\n",
    "            \n",
    "            if (output_dir / 'quant.sf').exists():\n",
    "                self.logger.info(f\"Quantification for {sample_name} already exists\")\n",
    "                continue\n",
    "            \n",
    "            if info['paired']:\n",
    "                read1 = self.dirs['trimmed'] / f\"{srr}_1.trimmed.fastq\"\n",
    "                read2 = self.dirs['trimmed'] / f\"{srr}_2.trimmed.fastq\"\n",
    "                \n",
    "                cmd = [\n",
    "                    'salmon', 'quant',\n",
    "                    '-i', str(index_dir),\n",
    "                    '-l', 'A',  # Auto-detect library type\n",
    "                    '-1', str(read1),\n",
    "                    '-2', str(read2),\n",
    "                    '-p', str(self.threads),\n",
    "                    '--validateMappings',\n",
    "                    '--gcBias',\n",
    "                    '--seqBias',\n",
    "                    '-o', str(output_dir)\n",
    "                ]\n",
    "            else:\n",
    "                read_file = self.dirs['trimmed'] / f\"{srr}.trimmed.fastq\"\n",
    "                cmd = [\n",
    "                    'salmon', 'quant',\n",
    "                    '-i', str(index_dir),\n",
    "                    '-l', 'A',\n",
    "                    '-r', str(read_file),\n",
    "                    '-p', str(self.threads),\n",
    "                    '--validateMappings',\n",
    "                    '--gcBias',\n",
    "                    '--seqBias',\n",
    "                    '-o', str(output_dir)\n",
    "                ]\n",
    "            \n",
    "            self.logger.info(f\"Quantifying {sample_name}...\")\n",
    "            try:\n",
    "                subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "                self.logger.info(f\"Successfully quantified {sample_name}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                self.logger.error(f\"Failed to quantify {sample_name}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def prepare_count_matrix(self):\n",
    "        \"\"\"Prepare count matrix from Salmon output\"\"\"\n",
    "        self.logger.info(\"Preparing count matrix...\")\n",
    "        \n",
    "        # Read transcript-to-gene mapping from GTF\n",
    "        gtf_file = self.dirs['references'] / 'Saccharomyces_cerevisiae.R64-1-1.107.gtf'\n",
    "        tx_to_gene = self.parse_gtf_for_tx_gene_mapping(gtf_file)\n",
    "        \n",
    "        # Collect all quantification results\n",
    "        sample_data = {}\n",
    "        \n",
    "        for srr, info in self.sample_info.items():\n",
    "            sample_name = f\"{info['condition']}_Rep{info['replicate']}\"\n",
    "            quant_file = self.dirs['counts'] / sample_name / 'quant.sf'\n",
    "            \n",
    "            if not quant_file.exists():\n",
    "                self.logger.error(f\"Quantification file not found for {sample_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Read Salmon output\n",
    "            df = pd.read_csv(quant_file, sep='\\t')\n",
    "            sample_data[sample_name] = df.set_index('Name')\n",
    "        \n",
    "        # Create transcript-level matrix\n",
    "        transcript_tpm = pd.DataFrame({sample: data['TPM'] for sample, data in sample_data.items()})\n",
    "        transcript_counts = pd.DataFrame({sample: data['NumReads'] for sample, data in sample_data.items()})\n",
    "        \n",
    "        # Aggregate to gene level\n",
    "        gene_tpm = self.aggregate_transcripts_to_genes(transcript_tpm, tx_to_gene)\n",
    "        gene_counts = self.aggregate_transcripts_to_genes(transcript_counts, tx_to_gene)\n",
    "        \n",
    "        # Save matrices\n",
    "        gene_tpm.to_csv(self.dirs['counts'] / 'gene_tpm_matrix.csv')\n",
    "        gene_counts.to_csv(self.dirs['counts'] / 'gene_counts_matrix.csv')\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = pd.DataFrame([\n",
    "            {\n",
    "                'sample': f\"{info['condition']}_Rep{info['replicate']}\",\n",
    "                'condition': info['condition'],\n",
    "                'replicate': info['replicate'],\n",
    "                'srr': srr\n",
    "            }\n",
    "            for srr, info in self.sample_info.items()\n",
    "        ])\n",
    "        \n",
    "        metadata.to_csv(self.dirs['counts'] / 'sample_metadata.csv', index=False)\n",
    "        \n",
    "        self.logger.info(\"Count matrices prepared successfully\")\n",
    "        return gene_tpm, gene_counts, metadata\n",
    "    \n",
    "    def parse_gtf_for_tx_gene_mapping(self, gtf_file):\n",
    "        \"\"\"Parse GTF file to get transcript-to-gene mapping\"\"\"\n",
    "        tx_to_gene = {}\n",
    "        \n",
    "        with open(gtf_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) < 9 or fields[2] != 'transcript':\n",
    "                    continue\n",
    "                \n",
    "                attributes = fields[8]\n",
    "                \n",
    "                # Parse attributes\n",
    "                gene_id = None\n",
    "                transcript_id = None\n",
    "                \n",
    "                for attr in attributes.split(';'):\n",
    "                    attr = attr.strip()\n",
    "                    if attr.startswith('gene_id'):\n",
    "                        gene_id = attr.split('\"')[1]\n",
    "                    elif attr.startswith('transcript_id'):\n",
    "                        transcript_id = attr.split('\"')[1]\n",
    "                \n",
    "                if gene_id and transcript_id:\n",
    "                    tx_to_gene[transcript_id] = gene_id\n",
    "        \n",
    "        return tx_to_gene\n",
    "    \n",
    "    def aggregate_transcripts_to_genes(self, transcript_matrix, tx_to_gene):\n",
    "        \"\"\"Aggregate transcript-level data to gene level\"\"\"\n",
    "        gene_matrix = {}\n",
    "        \n",
    "        for transcript_id, values in transcript_matrix.iterrows():\n",
    "            if transcript_id in tx_to_gene:\n",
    "                gene_id = tx_to_gene[transcript_id]\n",
    "                if gene_id not in gene_matrix:\n",
    "                    gene_matrix[gene_id] = values\n",
    "                else:\n",
    "                    gene_matrix[gene_id] += values\n",
    "        \n",
    "        return pd.DataFrame(gene_matrix).T\n",
    "    \n",
    "    def differential_expression_analysis(self, gene_counts, metadata):\n",
    "        \"\"\"Perform differential expression analysis using simple statistical methods\"\"\"\n",
    "        self.logger.info(\"Performing differential expression analysis...\")\n",
    "        \n",
    "        # Simple fold-change and t-test based analysis\n",
    "        conditions = metadata['condition'].unique()\n",
    "        \n",
    "        # Normalize counts (simple TPM-like normalization)\n",
    "        gene_lengths = gene_counts.sum(axis=1)  # Approximate gene length\n",
    "        normalized_counts = gene_counts.div(gene_lengths, axis=0) * 1e6\n",
    "        \n",
    "        # Log transform\n",
    "        log_counts = np.log2(normalized_counts + 1)\n",
    "        \n",
    "        # Perform pairwise comparisons\n",
    "        de_results = {}\n",
    "        \n",
    "        for condition in conditions:\n",
    "            if condition == 'Control':  # Skip if control condition exists\n",
    "                continue\n",
    "                \n",
    "            condition_samples = metadata[metadata['condition'] == condition]['sample'].values\n",
    "            other_samples = metadata[metadata['condition'] != condition]['sample'].values\n",
    "            \n",
    "            if len(other_samples) == 0:\n",
    "                continue\n",
    "            \n",
    "            condition_expr = log_counts[condition_samples].mean(axis=1)\n",
    "            other_expr = log_counts[other_samples].mean(axis=1)\n",
    "            \n",
    "            fold_change = condition_expr - other_expr\n",
    "            \n",
    "            # Simple statistical test (t-test approximation)\n",
    "            condition_var = log_counts[condition_samples].var(axis=1, ddof=1)\n",
    "            other_var = log_counts[other_samples].var(axis=1, ddof=1)\n",
    "            \n",
    "            # Handle division by zero\n",
    "            condition_var = condition_var.fillna(0.01)\n",
    "            other_var = other_var.fillna(0.01)\n",
    "            \n",
    "            pooled_se = np.sqrt((condition_var / len(condition_samples)) + \n",
    "                               (other_var / len(other_samples)))\n",
    "            \n",
    "            t_stat = fold_change / (pooled_se + 1e-8)  # Add small value to avoid division by zero\n",
    "            \n",
    "            # Create results dataframe\n",
    "            results_df = pd.DataFrame({\n",
    "                'gene': fold_change.index,\n",
    "                'log2FoldChange': fold_change,\n",
    "                'condition_mean': condition_expr,\n",
    "                'other_mean': other_expr,\n",
    "                't_stat': t_stat,\n",
    "                'abs_t_stat': np.abs(t_stat)\n",
    "            })\n",
    "            \n",
    "            results_df = results_df.sort_values('abs_t_stat', ascending=False)\n",
    "            de_results[condition] = results_df\n",
    "            \n",
    "            # Save results\n",
    "            results_df.to_csv(self.dirs['results'] / f'DE_{condition}_vs_others.csv', index=False)\n",
    "        \n",
    "        # Create feature matrix for ML\n",
    "        feature_matrix = pd.DataFrame(index=gene_counts.index)\n",
    "        \n",
    "        for condition, results in de_results.items():\n",
    "            feature_matrix[f'log2FC_{condition}'] = results.set_index('gene')['log2FoldChange']\n",
    "        \n",
    "        feature_matrix = feature_matrix.fillna(0)\n",
    "        feature_matrix.to_csv(self.dirs['results'] / 'ml_feature_matrix.csv')\n",
    "        \n",
    "        # Save normalized counts for ML\n",
    "        log_counts.to_csv(self.dirs['results'] / 'normalized_log_counts.csv')\n",
    "        \n",
    "        self.logger.info(\"Differential expression analysis completed\")\n",
    "        return de_results, feature_matrix, log_counts\n",
    "    \n",
    "    def create_de_visualizations(self, de_results, log_counts, metadata):\n",
    "        \"\"\"Create differential expression visualizations\"\"\"\n",
    "        self.logger.info(\"Creating differential expression visualizations...\")\n",
    "        \n",
    "        # PCA plot\n",
    "        from sklearn.decomposition import PCA\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(log_counts.T)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = ['red', 'blue', 'green', 'orange']\n",
    "        conditions = metadata['condition'].unique()\n",
    "        \n",
    "        for i, condition in enumerate(conditions):\n",
    "            mask = metadata['condition'] == condition\n",
    "            sample_indices = metadata[mask].index\n",
    "            plt.scatter(pca_result[sample_indices, 0], pca_result[sample_indices, 1], \n",
    "                       c=colors[i % len(colors)], label=condition, s=100, alpha=0.7)\n",
    "            \n",
    "            # Add sample labels\n",
    "            for idx in sample_indices:\n",
    "                plt.annotate(metadata.loc[idx, 'sample'], \n",
    "                           (pca_result[idx, 0], pca_result[idx, 1]),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "        plt.title('PCA of Samples by Stress Condition')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dirs['plots'] / 'PCA_samples.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Volcano plots\n",
    "        for condition, results in de_results.items():\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            x = results['log2FoldChange']\n",
    "            y = results['abs_t_stat']\n",
    "            \n",
    "            # Color points based on significance and fold change\n",
    "            colors = ['gray'] * len(results)\n",
    "            for i, (fc, t) in enumerate(zip(x, y)):\n",
    "                if abs(fc) > 1.5 and t > 2:  # Significant up/down\n",
    "                    colors[i] = 'red' if fc > 0 else 'blue'\n",
    "                elif abs(fc) > 1.5:  # High fold change\n",
    "                    colors[i] = 'orange'\n",
    "                elif t > 2:  # High t-statistic\n",
    "                    colors[i] = 'green'\n",
    "            \n",
    "            plt.scatter(x, y, c=colors, alpha=0.6, s=20)\n",
    "            \n",
    "            # Add significance lines\n",
    "            plt.axvline(x=1.5, color='black', linestyle='--', alpha=0.5)\n",
    "            plt.axvline(x=-1.5, color='black', linestyle='--', alpha=0.5)\n",
    "            plt.axhline(y=2, color='black', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            plt.xlabel('Log2 Fold Change')\n",
    "            plt.ylabel('|t-statistic|')\n",
    "            plt.title(f'Volcano Plot - {condition} vs Others')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add legend\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [\n",
    "                Patch(facecolor='red', label='Upregulated (FC>1.5, |t|>2)'),\n",
    "                Patch(facecolor='blue', label='Downregulated (FC<-1.5, |t|>2)'),\n",
    "                Patch(facecolor='orange', label='High fold change (|FC|>1.5)'),\n",
    "                Patch(facecolor='green', label='High significance (|t|>2)'),\n",
    "                Patch(facecolor='gray', label='Not significant')\n",
    "            ]\n",
    "            plt.legend(handles=legend_elements, loc='upper right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.dirs['plots'] / f'volcano_{condition}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # Heatmap of top variable genes\n",
    "        top_var_genes = log_counts.var(axis=1).nlargest(100).index\n",
    "        heatmap_data = log_counts.loc[top_var_genes]\n",
    "        \n",
    "        # Z-score normalization\n",
    "        heatmap_data = (heatmap_data - heatmap_data.mean(axis=1, keepdims=True)) / heatmap_data.std(axis=1, keepdims=True)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Create condition color map\n",
    "        condition_colors = dict(zip(conditions, colors[:len(conditions)]))\n",
    "        col_colors = [condition_colors[cond] for cond in metadata['condition']]\n",
    "        \n",
    "        sns.clustermap(heatmap_data, \n",
    "                      col_colors=col_colors,\n",
    "                      cmap='RdBu_r',\n",
    "                      center=0,\n",
    "                      figsize=(15, 10),\n",
    "                      cbar_kws={'label': 'Z-score'})\n",
    "        \n",
    "        plt.savefig(self.dirs['plots'] / 'heatmap_top100_genes.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        self.logger.info(\"Differential expression visualizations completed\")\n",
    "    \n",
    "    def machine_learning_analysis(self, log_counts, feature_matrix, metadata):\n",
    "        \"\"\"Perform comprehensive machine learning analysis\"\"\"\n",
    "        self.logger.info(\"Starting machine learning analysis...\")\n",
    "        \n",
    "        # Prepare data for sample classification\n",
    "        X_samples = log_counts.T  # Samples as rows, genes as columns\n",
    "        y_samples = metadata['condition'].values\n",
    "        \n",
    "        # Prepare data for gene classification\n",
    "        X_genes = feature_matrix.values\n",
    "        y_genes = self._create_gene_labels(feature_matrix)\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler_samples = StandardScaler()\n",
    "        X_samples_scaled = scaler_samples.fit_transform(X_samples)\n",
    "        \n",
    "        scaler_genes = StandardScaler()\n",
    "        X_genes_scaled = scaler_genes.fit_transform(X_genes)\n",
    "        \n",
    "        # Sample-level analysis\n",
    "        sample_results = self._train_classifiers(X_samples_scaled, y_samples, \"sample_classification\")\n",
    "        \n",
    "        # Gene-level analysis (if we have enough different labels)\n",
    "        if len(np.unique(y_genes)) > 2:\n",
    "            gene_results = self._train_classifiers(X_genes_scaled, y_genes, \"gene_classification\")\n",
    "        else:\n",
    "            gene_results = None\n",
    "        \n",
    "        # Dimensionality reduction and visualization\n",
    "        self._create_ml_visualizations(X_samples_scaled, y_samples, X_genes_scaled, y_genes, metadata)\n",
    "        \n",
    "        # Feature importance analysis\n",
    "        feature_importance = self._analyze_feature_importance(sample_results, log_counts.index)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        self._generate_ml_report(sample_results, gene_results, feature_importance, metadata)\n",
    "        \n",
    "        return sample_results, gene_results, feature_importance\n",
    "    \n",
    "    def _create_gene_labels(self, feature_matrix):\n",
    "        \"\"\"Create gene labels based on expression patterns\"\"\"\n",
    "        labels = []\n",
    "        \n",
    "        for idx, row in feature_matrix.iterrows():\n",
    "            max_fc = row.abs().max()\n",
    "            if max_fc > 2:  # Strong response\n",
    "                max_condition = row.abs().idxmax().replace('log2FC_', '')\n",
    "                labels.append(f\"{max_condition}_responsive\")\n",
    "            elif max_fc > 1:  # Moderate response\n",
    "                labels.append(\"moderate_response\")\n",
    "            else:\n",
    "                labels.append(\"low_response\")\n",
    "        \n",
    "        return np.array(labels)\n",
    "    \n",
    "    def _train_classifiers(self, X, y, analysis_type):\n",
    "        \"\"\"Train multiple classifiers and evaluate performance\"\"\"\n",
    "        self.logger.info(f\"Training classifiers for {analysis_type}...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Define classifiers\n",
    "        classifiers = {\n",
    "            'Random_Forest': RandomForestClassifier(n_estimators=500, random_state=42),\n",
    "            'XGBoost': XGBClassifier(n_estimators=300, random_state=42, eval_metric='mlogloss'),\n",
    "            'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            'Gradient_Boosting': GradientBoostingClassifier(n_estimators=300, random_state=42),\n",
    "            'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, clf in classifiers.items():\n",
    "            self.logger.info(f\"Training {name} for {analysis_type}...\")\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            \n",
    "            # Fit and predict\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_proba = clf.predict_proba(X_test) if hasattr(clf, 'predict_proba') else None\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'model': clf,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_proba,\n",
    "                'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "            }\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', \n",
    "                       xticklabels=np.unique(y), yticklabels=np.unique(y),\n",
    "                       cmap='Blues')\n",
    "            plt.title(f'{name} - Confusion Matrix ({analysis_type})')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.dirs['ml_results'] / f'{analysis_type}_{name}_confusion_matrix.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Save model\n",
    "            joblib.dump(clf, self.dirs['ml_results'] / f'{analysis_type}_{name}_model.joblib')\n",
    "        \n",
    "        # Model comparison\n",
    "        comparison_data = []\n",
    "        for name, res in results.items():\n",
    "            comparison_data.append({\n",
    "                'Model': name,\n",
    "                'CV_Mean': res['cv_mean'],\n",
    "                'CV_Std': res['cv_std'], \n",
    "                'Test_Accuracy': res['test_accuracy']\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df.to_csv(self.dirs['ml_results'] / f'{analysis_type}_model_comparison.csv', index=False)\n",
    "        \n",
    "        # Plot model comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.bar(comparison_df['Model'], comparison_df['CV_Mean'], \n",
    "                yerr=comparison_df['CV_Std'], capsize=5)\n",
    "        plt.xlabel('Classifier')\n",
    "        plt.ylabel('Cross-Validation Accuracy')\n",
    "        plt.title(f'Cross-Validation Performance ({analysis_type})')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.bar(comparison_df['Model'], comparison_df['Test_Accuracy'])\n",
    "        plt.xlabel('Classifier')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title(f'Test Performance ({analysis_type})')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dirs['ml_results'] / f'{analysis_type}_model_comparison.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _create_ml_visualizations(self, X_samples, y_samples, X_genes, y_genes, metadata):\n",
    "        \"\"\"Create comprehensive ML visualizations\"\"\"\n",
    "        self.logger.info(\"Creating ML visualizations...\")\n",
    "        \n",
    "        # PCA for samples\n",
    "        pca_samples = PCA(n_components=2)\n",
    "        pcs_samples = pca_samples.fit_transform(X_samples)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Sample PCA\n",
    "        plt.subplot(1, 3, 1)\n",
    "        colors = ['red', 'blue', 'green', 'orange']\n",
    "        for i, condition in enumerate(np.unique(y_samples)):\n",
    "            mask = y_samples == condition\n",
    "            plt.scatter(pcs_samples[mask, 0], pcs_samples[mask, 1], \n",
    "                       c=colors[i], label=condition, s=100, alpha=0.7)\n",
    "        \n",
    "        plt.xlabel(f'PC1 ({pca_samples.explained_variance_ratio_[0]:.1%})')\n",
    "        plt.ylabel(f'PC2 ({pca_samples.explained_variance_ratio_[1]:.1%})')\n",
    "        plt.title('PCA - Samples by Condition')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # UMAP for samples\n",
    "        plt.subplot(1, 3, 2)\n",
    "        reducer_samples = umap.UMAP(n_neighbors=5, min_dist=0.1, random_state=42)\n",
    "        embedding_samples = reducer_samples.fit_transform(X_samples)\n",
    "        \n",
    "        for i, condition in enumerate(np.unique(y_samples)):\n",
    "            mask = y_samples == condition\n",
    "            plt.scatter(embedding_samples[mask, 0], embedding_samples[mask, 1], \n",
    "                       c=colors[i], label=condition, s=100, alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('UMAP1')\n",
    "        plt.ylabel('UMAP2')\n",
    "        plt.title('UMAP - Samples by Condition')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # t-SNE for samples\n",
    "        plt.subplot(1, 3, 3)\n",
    "        tsne_samples = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_samples)-1))\n",
    "        tsne_result = tsne_samples.fit_transform(X_samples)\n",
    "        \n",
    "        for i, condition in enumerate(np.unique(y_samples)):\n",
    "            mask = y_samples == condition\n",
    "            plt.scatter(tsne_result[mask, 0], tsne_result[mask, 1], \n",
    "                       c=colors[i], label=condition, s=100, alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('t-SNE1')\n",
    "        plt.ylabel('t-SNE2')\n",
    "        plt.title('t-SNE - Samples by Condition')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dirs['ml_results'] / 'dimensionality_reduction_samples.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Gene-level visualizations if applicable\n",
    "        if len(np.unique(y_genes)) > 2:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "            # PCA for genes\n",
    "            plt.subplot(1, 2, 1)\n",
    "            pca_genes = PCA(n_components=2)\n",
    "            pcs_genes = pca_genes.fit_transform(X_genes)\n",
    "            \n",
    "            unique_gene_labels = np.unique(y_genes)\n",
    "            colors_genes = plt.cm.Set3(np.linspace(0, 1, len(unique_gene_labels)))\n",
    "            \n",
    "            for i, label in enumerate(unique_gene_labels):\n",
    "                mask = y_genes == label\n",
    "                plt.scatter(pcs_genes[mask, 0], pcs_genes[mask, 1], \n",
    "                           c=[colors_genes[i]], label=label, alpha=0.6, s=20)\n",
    "            \n",
    "            plt.xlabel(f'PC1 ({pca_genes.explained_variance_ratio_[0]:.1%})')\n",
    "            plt.ylabel(f'PC2 ({pca_genes.explained_variance_ratio_[1]:.1%})')\n",
    "            plt.title('PCA - Genes by Response Pattern')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # UMAP for genes\n",
    "            plt.subplot(1, 2, 2)\n",
    "            if len(X_genes) > 50:  # Only if we have enough genes\n",
    "                reducer_genes = umap.UMAP(n_neighbors=min(15, len(X_genes)//3), \n",
    "                                        min_dist=0.1, random_state=42)\n",
    "                embedding_genes = reducer_genes.fit_transform(X_genes)\n",
    "                \n",
    "                for i, label in enumerate(unique_gene_labels):\n",
    "                    mask = y_genes == label\n",
    "                    plt.scatter(embedding_genes[mask, 0], embedding_genes[mask, 1], \n",
    "                               c=[colors_genes[i]], label=label, alpha=0.6, s=20)\n",
    "                \n",
    "                plt.xlabel('UMAP1')\n",
    "                plt.ylabel('UMAP2')\n",
    "                plt.title('UMAP - Genes by Response Pattern')\n",
    "                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.dirs['ml_results'] / 'dimensionality_reduction_genes.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    def _analyze_feature_importance(self, sample_results, gene_names):\n",
    "        \"\"\"Analyze feature importance from the best performing model\"\"\"\n",
    "        self.logger.info(\"Analyzing feature importance...\")\n",
    "        \n",
    "        # Use Random Forest for feature importance (most interpretable)\n",
    "        if 'Random_Forest' in sample_results:\n",
    "            rf_model = sample_results['Random_Forest']['model']\n",
    "            importances = rf_model.feature_importances_\n",
    "            \n",
    "            feature_importance = pd.DataFrame({\n",
    "                'gene': gene_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Save feature importance\n",
    "            feature_importance.to_csv(self.dirs['ml_results'] / 'feature_importance.csv', index=False)\n",
    "            \n",
    "            # Plot top features\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            top_features = feature_importance.head(30)\n",
    "            \n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['gene'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title('Top 30 Most Important Genes for Condition Classification')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.dirs['ml_results'] / 'feature_importance_top30.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Feature importance distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(importances, bins=50, alpha=0.7, edgecolor='black')\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.ylabel('Number of Genes')\n",
    "            plt.title('Distribution of Feature Importances')\n",
    "            plt.axvline(importances.mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: {importances.mean():.4f}')\n",
    "            plt.axvline(np.percentile(importances, 95), color='orange', linestyle='--', \n",
    "                       label=f'95th percentile: {np.percentile(importances, 95):.4f}')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.dirs['ml_results'] / 'feature_importance_distribution.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            return feature_importance\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _generate_ml_report(self, sample_results, gene_results, feature_importance, metadata):\n",
    "        \"\"\"Generate comprehensive ML analysis report\"\"\"\n",
    "        self.logger.info(\"Generating ML analysis report...\")\n",
    "        \n",
    "        report_content = f\"\"\"# Fungal Gene Expression ML Analysis Report\n",
    "\n",
    "## Project Overview\n",
    "- **Organism**: Saccharomyces cerevisiae\n",
    "- **Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Total Samples**: {len(metadata)}\n",
    "- **Stress Conditions**: {', '.join(metadata['condition'].unique())}\n",
    "- **Samples per Condition**: {dict(metadata['condition'].value_counts())}\n",
    "\n",
    "## Dataset Summary\n",
    "- **Total Genes Analyzed**: {len(feature_importance) if feature_importance is not None else 'N/A'}\n",
    "- **Features Used**: Gene expression levels across conditions\n",
    "- **Analysis Types**: Sample classification, Feature importance analysis\n",
    "\n",
    "## Sample Classification Results\n",
    "\n",
    "### Model Performance Summary\n",
    "\"\"\"\n",
    "        \n",
    "        if sample_results:\n",
    "            for name, results in sample_results.items():\n",
    "                report_content += f\"\"\"\n",
    "#### {name.replace('_', ' ')}\n",
    "- **Cross-validation Accuracy**: {results['cv_mean']:.3f} ± {results['cv_std']:.3f}\n",
    "- **Test Accuracy**: {results['test_accuracy']:.3f}\n",
    "\"\"\"\n",
    "        \n",
    "        if feature_importance is not None:\n",
    "            top_genes = feature_importance.head(10)\n",
    "            report_content += f\"\"\"\n",
    "## Top 10 Biomarker Genes\n",
    "\n",
    "| Rank | Gene ID | Importance Score |\n",
    "|------|---------|-----------------|\n",
    "\"\"\"\n",
    "            for i, (_, row) in enumerate(top_genes.iterrows(), 1):\n",
    "                report_content += f\"| {i} | {row['gene']} | {row['importance']:.4f} |\\n\"\n",
    "        \n",
    "        if gene_results:\n",
    "            report_content += f\"\"\"\n",
    "## Gene Classification Results\n",
    "\n",
    "Gene-level classification was performed to identify genes with similar response patterns.\n",
    "\"\"\"\n",
    "            for name, results in gene_results.items():\n",
    "                report_content += f\"\"\"\n",
    "### {name.replace('_', ' ')}\n",
    "- **Test Accuracy**: {results['test_accuracy']:.3f}\n",
    "\"\"\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "## Key Findings\n",
    "\n",
    "### 1. Condition Separability\n",
    "- Stress conditions show {'distinct' if len(metadata['condition'].unique()) > 2 else 'overlapping'} expression signatures\n",
    "- Best classification accuracy: {max([r['test_accuracy'] for r in sample_results.values()]) if sample_results else 'N/A':.1%}\n",
    "\n",
    "### 2. Biomarker Discovery\n",
    "\"\"\"\n",
    "        \n",
    "        if feature_importance is not None:\n",
    "            high_importance_genes = len(feature_importance[feature_importance['importance'] > feature_importance['importance'].quantile(0.95)])\n",
    "            report_content += f\"\"\"- **{high_importance_genes}** genes show high discriminative power (>95th percentile)\n",
    "- Top biomarker: **{feature_importance.iloc[0]['gene']}** (importance: {feature_importance.iloc[0]['importance']:.4f})\n",
    "\"\"\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "### 3. Model Performance\n",
    "- **Random Forest**: Generally most interpretable and robust\n",
    "- **XGBoost**: Often highest accuracy but less interpretable\n",
    "- **SVM**: Good for high-dimensional data with clear boundaries\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "### Data Processing Pipeline\n",
    "1. **Data Acquisition**: SRA samples downloaded and processed\n",
    "2. **Quality Control**: FastQC and adapter trimming\n",
    "3. **Quantification**: Salmon quasi-mapping for transcript quantification\n",
    "4. **Normalization**: Log2 transformation and standardization\n",
    "5. **Feature Engineering**: Fold-change calculations across conditions\n",
    "6. **Machine Learning**: Multiple classifier training and evaluation\n",
    "\n",
    "### Files Generated\n",
    "- `feature_importance.csv`: Gene importance rankings\n",
    "- `*_model_comparison.csv`: Model performance metrics\n",
    "- `*.joblib`: Trained models for future use\n",
    "- Various visualization plots in PNG format\n",
    "\n",
    "## Biological Implications\n",
    "\n",
    "The identified biomarker genes likely represent key regulators of stress response in yeast.\n",
    "These findings could guide:\n",
    "- **Drug Target Discovery**: Focus on highly important genes\n",
    "- **Biomarker Development**: Use top genes for stress condition prediction\n",
    "- **Functional Studies**: Investigate biological roles of key genes\n",
    "\n",
    "## Recommendations for Future Work\n",
    "\n",
    "1. **Experimental Validation**: Test top biomarker genes experimentally\n",
    "2. **Pathway Analysis**: Perform functional enrichment on important genes\n",
    "3. **Time-course Studies**: Analyze temporal expression patterns\n",
    "4. **Cross-species Validation**: Test biomarkers in related fungal species\n",
    "5. **Deep Learning**: Implement neural networks for sequence-based features\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This ML-based analysis successfully identified condition-specific biomarkers in yeast stress response.\n",
    "The pipeline demonstrates the power of combining RNA-seq with machine learning for biological discovery.\n",
    "\"\"\"\n",
    "        \n",
    "        # Save report\n",
    "        with open(self.dirs['ml_results'] / 'comprehensive_analysis_report.md', 'w') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        # Create summary statistics\n",
    "        summary_stats = {\n",
    "            'total_samples': len(metadata),\n",
    "            'total_conditions': len(metadata['condition'].unique()),\n",
    "            'best_accuracy': max([r['test_accuracy'] for r in sample_results.values()]) if sample_results else None,\n",
    "            'top_biomarker': feature_importance.iloc[0]['gene'] if feature_importance is not None else None,\n",
    "            'high_importance_genes': len(feature_importance[feature_importance['importance'] > feature_importance['importance'].quantile(0.95)]) if feature_importance is not None else None\n",
    "        }\n",
    "        \n",
    "        with open(self.dirs['ml_results'] / 'summary_statistics.json', 'w') as f:\n",
    "            json.dump(summary_stats, f, indent=2)\n",
    "        \n",
    "        self.logger.info(\"ML analysis report generated successfully\")\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Run the complete pipeline from start to finish\"\"\"\n",
    "        self.logger.info(\"=\"*60)\n",
    "        self.logger.info(\"STARTING COMPLETE FUNGAL RNA-SEQ ML PIPELINE\")\n",
    "        self.logger.info(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Check dependencies\n",
    "            if not self.check_dependencies():\n",
    "                self.logger.error(\"Missing dependencies. Please install required tools.\")\n",
    "                return False\n",
    "            \n",
    "            # Step 2: Download reference files\n",
    "            if not self.download_references():\n",
    "                self.logger.error(\"Failed to download reference files\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Download SRA data\n",
    "            if not self.download_sra_data():\n",
    "                self.logger.error(\"Failed to download SRA data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 4: Quality control\n",
    "            if not self.quality_control():\n",
    "                self.logger.error(\"Quality control failed\")\n",
    "                return False\n",
    "            \n",
    "            # Step 5: Trim adapters\n",
    "            if not self.trim_adapters():\n",
    "                self.logger.error(\"Adapter trimming failed\")\n",
    "                return False\n",
    "            \n",
    "            # Step 6: Build Salmon index\n",
    "            if not self.build_salmon_index():\n",
    "                self.logger.error(\"Failed to build Salmon index\")\n",
    "                return False\n",
    "            \n",
    "            # Step 7: Quantify expression\n",
    "            if not self.quantify_expression():\n",
    "                self.logger.error(\"Expression quantification failed\")\n",
    "                return False\n",
    "            \n",
    "            # Step 8: Prepare count matrices\n",
    "            gene_tpm, gene_counts, metadata = self.prepare_count_matrix()\n",
    "            if gene_tpm is None:\n",
    "                self.logger.error(\"Failed to prepare count matrix\")\n",
    "                return False\n",
    "            \n",
    "            # Step 9: Differential expression analysis\n",
    "            de_results, feature_matrix, log_counts = self.differential_expression_analysis(gene_counts, metadata)\n",
    "            \n",
    "            # Step 10: Create DE visualizations\n",
    "            self.create_de_visualizations(de_results, log_counts, metadata)\n",
    "            \n",
    "            # Step 11: Machine learning analysis\n",
    "            sample_results, gene_results, feature_importance = self.machine_learning_analysis(\n",
    "                log_counts, feature_matrix, metadata\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\"=\"*60)\n",
    "            self.logger.info(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "            self.logger.info(\"=\"*60)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\n📊 ANALYSIS SUMMARY:\")\n",
    "            print(f\"├── Samples processed: {len(metadata)}\")\n",
    "            print(f\"├── Conditions analyzed: {len(metadata['condition'].unique())}\")\n",
    "            print(f\"├── Genes quantified: {len(gene_counts)}\")\n",
    "            if sample_results:\n",
    "                best_model = max(sample_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "                print(f\"├── Best ML model: {best_model[0]} ({best_model[1]['test_accuracy']:.1%} accuracy)\")\n",
    "            if feature_importance is not None:\n",
    "                print(f\"├── Top biomarker: {feature_importance.iloc[0]['gene']}\")\n",
    "            print(f\"└── Results saved in: {self.project_dir}\")\n",
    "            \n",
    "            print(f\"\\n📁 KEY OUTPUT FILES:\")\n",
    "            print(f\"├── 📈 ML Results: {self.dirs['ml_results']}\")\n",
    "            print(f\"├── 📊 Plots: {self.dirs['plots']}\")\n",
    "            print(f\"├── 📋 Reports: comprehensive_analysis_report.md\")\n",
    "            print(f\"├── 🧬 Gene Data: gene_tpm_matrix.csv, gene_counts_matrix.csv\")\n",
    "            print(f\"└── 🤖 Models: *.joblib files\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed with error: {e}\")\n",
    "            import traceback\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the pipeline\"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Complete Fungal RNA-seq ML Pipeline')\n",
    "    parser.add_argument('--project-dir', default='yeast_rnaseq_ml_project',\n",
    "                       help='Project directory name (default: yeast_rnaseq_ml_project)')\n",
    "    parser.add_argument('--threads', type=int, default=8,\n",
    "                       help='Number of threads to use (default: 8)')\n",
    "    parser.add_argument('--skip-download', action='store_true',\n",
    "                       help='Skip data download if files already exist')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = FungalRNAseqMLPipeline(\n",
    "        project_dir=args.project_dir,\n",
    "        threads=args.threads\n",
    "    )\n",
    "    \n",
    "    # Run complete pipeline\n",
    "    success = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 Pipeline completed successfully!\")\n",
    "        print(\"📖 Check the comprehensive_analysis_report.md for detailed results!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Pipeline failed. Check the logs for details.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d9ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
